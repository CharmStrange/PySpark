{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBbSG8KxoFkaVIGYJCLxYn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharmStrange/PySpark/blob/main/PySpark_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM7ZmHdduQuc",
        "outputId": "3f64bab4-073e-47e3-abac-c49d6dde955e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=4b07d3c2de202c102a441736825c709f1363fa50cc46a8473ccdb09566810a2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.session import SparkSession"
      ],
      "metadata": {
        "id": "9ZexG9yjuWK8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .master(\"local\") \\\n",
        "    .appName(\"Word Count\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "a5yL9zh0u4lK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "4y9xFxsqvlAc",
        "outputId": "2f7dd712-137f-4bed-907d-e50456bc21b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.session.SparkSession"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.session.SparkSession</b><br/>def __init__(sparkContext: SparkContext, jsparkSession: Optional[JavaObject]=None, options: Dict[str, Any]={})</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py</a>The entry point to programming Spark with the Dataset and DataFrame API.\n",
              "\n",
              "A SparkSession can be used to create :class:`DataFrame`, register :class:`DataFrame` as\n",
              "tables, execute SQL over tables, cache tables, and read parquet files.\n",
              "To create a :class:`SparkSession`, use the following builder pattern:\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              ".. autoattribute:: builder\n",
              "   :annotation:\n",
              "\n",
              "Examples\n",
              "--------\n",
              "Create a Spark session.\n",
              "\n",
              "&gt;&gt;&gt; spark = (\n",
              "...     SparkSession.builder\n",
              "...         .master(&quot;local&quot;)\n",
              "...         .appName(&quot;Word Count&quot;)\n",
              "...         .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)\n",
              "...         .getOrCreate()\n",
              "... )\n",
              "\n",
              "Create a Spark session with Spark Connect.\n",
              "\n",
              "&gt;&gt;&gt; spark = (\n",
              "...     SparkSession.builder\n",
              "...         .remote(&quot;sc://localhost&quot;)\n",
              "...         .appName(&quot;Word Count&quot;)\n",
              "...         .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)\n",
              "...         .getOrCreate()\n",
              "... )  # doctest: +SKIP</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 166);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.context import SparkContext\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "mowcujBavmSl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(sc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "-ZRK43IlvrVV",
        "outputId": "4aee0409-e832-4b11-d5bb-ebf07cdc5865"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.context.SparkContext"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.context.SparkContext</b><br/>def __init__(master: Optional[str]=None, appName: Optional[str]=None, sparkHome: Optional[str]=None, pyFiles: Optional[List[str]]=None, environment: Optional[Dict[str, Any]]=None, batchSize: int=0, serializer: &#x27;Serializer&#x27;=CPickleSerializer(), conf: Optional[SparkConf]=None, gateway: Optional[JavaGateway]=None, jsc: Optional[JavaObject]=None, profiler_cls: Type[BasicProfiler]=BasicProfiler, udf_profiler_cls: Type[UDFBasicProfiler]=UDFBasicProfiler, memory_profiler_cls: Type[MemoryProfiler]=MemoryProfiler)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/context.py</a>Main entry point for Spark functionality. A SparkContext represents the\n",
              "connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
              "broadcast variables on that cluster.\n",
              "\n",
              "When you create a new SparkContext, at least the master and app name should\n",
              "be set, either through the named parameters here or through `conf`.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "master : str, optional\n",
              "    Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).\n",
              "appName : str, optional\n",
              "    A name for your job, to display on the cluster web UI.\n",
              "sparkHome : str, optional\n",
              "    Location where Spark is installed on cluster nodes.\n",
              "pyFiles : list, optional\n",
              "    Collection of .zip or .py files to send to the cluster\n",
              "    and add to PYTHONPATH.  These can be paths on the local file\n",
              "    system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
              "environment : dict, optional\n",
              "    A dictionary of environment variables to set on\n",
              "    worker nodes.\n",
              "batchSize : int, optional, default 0\n",
              "    The number of Python objects represented as a single\n",
              "    Java object. Set 1 to disable batching, 0 to automatically choose\n",
              "    the batch size based on object sizes, or -1 to use an unlimited\n",
              "    batch size\n",
              "serializer : :class:`Serializer`, optional, default :class:`CPickleSerializer`\n",
              "    The serializer for RDDs.\n",
              "conf : :class:`SparkConf`, optional\n",
              "    An object setting Spark properties.\n",
              "gateway : class:`py4j.java_gateway.JavaGateway`,  optional\n",
              "    Use an existing gateway and JVM, otherwise a new JVM\n",
              "    will be instantiated. This is only used internally.\n",
              "jsc : class:`py4j.java_gateway.JavaObject`, optional\n",
              "    The JavaSparkContext instance. This is only used internally.\n",
              "profiler_cls : type, optional, default :class:`BasicProfiler`\n",
              "    A class of custom Profiler used to do profiling\n",
              "udf_profiler_cls : type, optional, default :class:`UDFBasicProfiler`\n",
              "    A class of custom Profiler used to do udf profiling\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n",
              "the active :class:`SparkContext` before creating a new one.\n",
              "\n",
              ":class:`SparkContext` instance is not supported to share across multiple\n",
              "processes out of the box, and PySpark does not guarantee multi-processing execution.\n",
              "Use threads instead for concurrent processing purpose.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; from pyspark.context import SparkContext\n",
              "&gt;&gt;&gt; sc = SparkContext(&#x27;local&#x27;, &#x27;test&#x27;)\n",
              "&gt;&gt;&gt; sc2 = SparkContext(&#x27;local&#x27;, &#x27;test2&#x27;) # doctest: +IGNORE_EXCEPTION_DETAIL\n",
              "Traceback (most recent call last):\n",
              "    ...\n",
              "ValueError: ...</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 92);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}