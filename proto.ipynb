{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHH3m1S4tJ4nW8EGlxtmzm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharmStrange/PySpark/blob/main/proto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://granulate.io/blog/5-pyspark-optimization-techniques-you-should-know/#Use-DataFrame-Dataset-over-RDD\n",
        "\n",
        "https://sparkbyexamples.com/pyspark-rdd/#what-is-rdd"
      ],
      "metadata": {
        "id": "Ap_83-ynRZmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sL6IufxSM37f",
        "outputId": "028b09cf-5949-4003-c623-ef3bf4fa219a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=40712155038a8b5b74643516b010284fcad18e02c95f97157c817c26e6bc7c52\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# 변수들을 딕셔너리에 저장\n",
        "regions = {\n",
        "    \"서울특별시\": \"https://www.daangn.com/region/%EC%84%9C%EC%9A%B8%ED%8A%B9%EB%B3%84%EC%8B%9C\",\n",
        "    \"부산광역시\": \"https://www.daangn.com/region/%EB%B6%80%EC%82%B0%EA%B4%91%EC%97%AD%EC%8B%9C\",\n",
        "    \"대구광역시\": \"https://www.daangn.com/region/%EB%8C%80%EA%B5%AC%EA%B4%91%EC%97%AD%EC%8B%9C\",\n",
        "    \"인천광역시\": \"https://www.daangn.com/region/%EC%9D%B8%EC%B2%9C%EA%B4%91%EC%97%AD%EC%8B%9C\",\n",
        "    \"광주광역시\": \"https://www.daangn.com/region/%EA%B4%91%EC%A3%BC%EA%B4%91%EC%97%AD%EC%8B%9C\",\n",
        "    \"대전광역시\": \"https://www.daangn.com/region/%EB%8C%80%EC%A0%84%EA%B4%91%EC%97%AD%EC%8B%9C\",\n",
        "    \"울산광역시\": \"https://www.daangn.com/region/%EC%9A%B8%EC%82%B0%EA%B4%91%EC%97%AD%EC%8B%9C\",\n",
        "    \"세종특별자치시\": \"https://www.daangn.com/region/%EC%84%B8%EC%A2%85%ED%8A%B9%EB%B3%84%EC%9E%90%EC%B9%98%EC%8B%9C\",\n",
        "    \"경기도\": \"https://www.daangn.com/region/%EA%B2%BD%EA%B8%B0%EB%8F%84\",\n",
        "    \"강원특별자치도\": \"https://www.daangn.com/region/%EA%B0%95%EC%9B%90%ED%8A%B9%EB%B3%84%EC%9E%90%EC%B9%98%EB%8F%84\",\n",
        "    \"충청북도\": \"https://www.daangn.com/region/%EC%B6%A9%EC%B2%AD%EB%B6%81%EB%8F%84\",\n",
        "    \"충청남도\": \"https://www.daangn.com/region/%EC%B6%A9%EC%B2%AD%EB%82%A8%EB%8F%84\",\n",
        "    \"전라북도\": \"https://www.daangn.com/region/%EC%A0%84%EB%9D%BC%EB%B6%81%EB%8F%84\",\n",
        "    \"전라남도\": \"https://www.daangn.com/region/%EC%A0%84%EB%9D%BC%EB%82%A8%EB%8F%84\",\n",
        "    \"경상북도\": \"https://www.daangn.com/region/%EA%B2%BD%EC%83%81%EB%B6%81%EB%8F%84\",\n",
        "    \"경상남도\": \"https://www.daangn.com/region/%EA%B2%BD%EC%83%81%EB%82%A8%EB%8F%84\",\n",
        "    \"제주특별자치도\": \"https://www.daangn.com/region/%EC%A0%9C%EC%A3%BC%ED%8A%B9%EB%B3%84%EC%9E%90%EC%B9%98%EB%8F%84\"\n",
        "}\n",
        "\n",
        "# 반복문을 통한 웹 크롤링 및 결과 저장\n",
        "for region, url in regions.items():\n",
        "    response = requests.get(url)\n",
        "    html_content = response.text\n",
        "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "    card_title_elements = soup.find_all(class_=\"card-title\")\n",
        "    card_price_elements = soup.find_all(class_=\"card-price\")\n",
        "    output_file = f\"list_{region}.txt\"\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for element in zip(card_title_elements, card_price_elements):\n",
        "            title = element[0].get_text(strip=True)\n",
        "            price = element[1].get_text(strip=True)\n",
        "            f.write(title +  ' / ' + price + \"\\n\")\n"
      ],
      "metadata": {
        "id": "QMtGVR90tsIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"Python Spark SQL basic example\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
      ],
      "metadata": {
        "id": "QPqF6oDQNDV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles = []\n",
        "for element in card_title_elements:\n",
        "  titles.append(element.get_text(strip=True))\n",
        "\n",
        "prices = []\n",
        "for element in card_price_elements:\n",
        "  prices.append(element.get_text(strip=True))\n",
        "\n",
        "rdd_titles = spark.sparkContext.parallelize(titles)\n",
        "rdd_prices = spark.sparkContext.parallelize(prices)"
      ],
      "metadata": {
        "id": "gTuetfNHNZ96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_titles, rdd_prices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9LZx2ZKQ1dD",
        "outputId": "16d920ff-f661-4c11-9016-4641a2cb11fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(ParallelCollectionRDD[2] at readRDDFromFile at PythonRDD.scala:289,\n",
              " ParallelCollectionRDD[3] at readRDDFromFile at PythonRDD.scala:289)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}