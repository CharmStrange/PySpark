{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpjRSBH2GmSELj/loRHYyP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharmStrange/PySpark/blob/main/proto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://granulate.io/blog/5-pyspark-optimization-techniques-you-should-know/#Use-DataFrame-Dataset-over-RDD\n",
        "\n",
        "https://sparkbyexamples.com/pyspark-rdd/#what-is-rdd"
      ],
      "metadata": {
        "id": "Ap_83-ynRZmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sL6IufxSM37f",
        "outputId": "4e5be8f7-75f8-48c0-ca38-32e7b7d9e5a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=975fc825dd078174a3d46b057e77dc82ec3092aa09c7d8ab220d576aa4e19889\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import sqlite3\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "regions = {\n",
        "    \"서울특별시\": \"https://www.daangn.com/region/%EC%84%9C%EC%9A%B8%ED%8A%B9%EB%B3%84%EC%8B%9C\",\n",
        "    \"부산광역시\": \"https://www.daangn.com/region/%EB%B6%80%EC%82%B0%EA%B4%91%EC%97%AD%EC%8B%9C\",\n",
        "    \"대구광역시\": \"https://www.daangn.com/region/%EB%8C%80%EA%B5%AC%EA%B4%91%EC%97%AD%EC%8B%9C\",\n",
        "    \"인천광역시\": \"https://www.daangn.com/region/%EC%9D%B8%EC%B2%9C%EA%B4%91%EC%97%AD%EC%8B%9C\",\n",
        "    \"광주광역시\": \"https://www.daangn.com/region/%EA%B4%91%EC%A3%BC%EA%B4%91%EC%97%AD%EC%8B%9C\",\n",
        "    \"대전광역시\": \"https://www.daangn.com/region/%EB%8C%80%EC%A0%84%EA%B4%91%EC%97%AD%EC%8B%9C\",\n",
        "    \"울산광역시\": \"https://www.daangn.com/region/%EC%9A%B8%EC%82%B0%EA%B4%91%EC%97%AD%EC%8B%9C\",\n",
        "    \"세종특별자치시\": \"https://www.daangn.com/region/%EC%84%B8%EC%A2%85%ED%8A%B9%EB%B3%84%EC%9E%90%EC%B9%98%EC%8B%9C\",\n",
        "    \"경기도\": \"https://www.daangn.com/region/%EA%B2%BD%EA%B8%B0%EB%8F%84\",\n",
        "    \"강원특별자치도\": \"https://www.daangn.com/region/%EA%B0%95%EC%9B%90%ED%8A%B9%EB%B3%84%EC%9E%90%EC%B9%98%EB%8F%84\",\n",
        "    \"충청북도\": \"https://www.daangn.com/region/%EC%B6%A9%EC%B2%AD%EB%B6%81%EB%8F%84\",\n",
        "    \"충청남도\": \"https://www.daangn.com/region/%EC%B6%A9%EC%B2%AD%EB%82%A8%EB%8F%84\",\n",
        "    \"전라북도\": \"https://www.daangn.com/region/%EC%A0%84%EB%9D%BC%EB%B6%81%EB%8F%84\",\n",
        "    \"전라남도\": \"https://www.daangn.com/region/%EC%A0%84%EB%9D%BC%EB%82%A8%EB%8F%84\",\n",
        "    \"경상북도\": \"https://www.daangn.com/region/%EA%B2%BD%EC%83%81%EB%B6%81%EB%8F%84\",\n",
        "    \"경상남도\": \"https://www.daangn.com/region/%EA%B2%BD%EC%83%81%EB%82%A8%EB%8F%84\",\n",
        "    \"제주특별자치도\": \"https://www.daangn.com/region/%EC%A0%9C%EC%A3%BC%ED%8A%B9%EB%B3%84%EC%9E%90%EC%B9%98%EB%8F%84\"\n",
        "}\n",
        "\n",
        "spark = SparkSession.builder.appName(\"proto\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
        "\n",
        "titles = []\n",
        "prices = []\n",
        "for region, url in regions.items():\n",
        "  response = requests.get(url)\n",
        "  html_content = response.text\n",
        "  soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "  card_title_elements = soup.find_all(class_=\"card-title\")\n",
        "  card_price_elements = soup.find_all(class_=\"card-price\")\n",
        "\n",
        "  for element in card_title_elements:\n",
        "    titles.append(element.get_text(strip=True))\n",
        "\n",
        "  for element in card_price_elements:\n",
        "    text = element.get_text(strip=True)\n",
        "    text = re.sub(r'[^0-9]', '', text)\n",
        "    print(text)\n",
        "    prices.append(text)\n",
        "\n",
        "\n",
        "rdd_titles = spark.sparkContext.parallelize(titles)\n",
        "rdd_prices = spark.sparkContext.parallelize(prices)"
      ],
      "metadata": {
        "id": "QMtGVR90tsIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rdd_titles.count())\n",
        "print(rdd_prices.count())\n",
        "\n",
        "print(rdd_titles.first())\n",
        "print(rdd_prices.first())\n",
        "\n",
        "rdd_titles.saveAsTextFile(\"rdd_titles\")\n",
        "rdd_prices.saveAsTextFile(\"rdd_prices\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9LZx2ZKQ1dD",
        "outputId": "ab92f57f-c7ca-44e3-936d-e7cf1ae41a0a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1684\n",
            "1684\n",
            "남편 몰래 팝니다\n",
            "200000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import os\n",
        "\n",
        "db_file = 'CarrotText_WHOLE.db'\n",
        "\n",
        "def create_table():\n",
        "    conn = sqlite3.connect('CarrotText_WHOLE.db')\n",
        "    c = conn.cursor()\n",
        "\n",
        "    c.execute('''CREATE TABLE IF NOT EXISTS CarrotTextWHOLE\n",
        "                 (Title TEXT, Price TEXT)''')\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def create_database():\n",
        "    if not os.path.exists(db_file):\n",
        "        create_table()\n",
        "\n",
        "def add_new_data(title, price):\n",
        "    conn = sqlite3.connect(db_file)\n",
        "    c = conn.cursor()\n",
        "\n",
        "    try:\n",
        "        c.execute('''INSERT INTO CarrotTextWHOLE (Title, Price) VALUES (?, ?)''', (title, price))\n",
        "    except sqlite3.Error as e:\n",
        "        print(\"Error occurred:\", e)\n",
        "    finally:\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "def insert_rdd_to_db(rdd_titles, rdd_prices):\n",
        "    titles = rdd_titles.collect()\n",
        "    prices = rdd_prices.collect()\n",
        "\n",
        "    for title, price in zip(titles, prices):\n",
        "        add_new_data(title, price)\n",
        "\n",
        "create_database()\n",
        "\n",
        "rdd_titles = spark.sparkContext.parallelize(titles)\n",
        "rdd_prices = spark.sparkContext.parallelize(prices)\n",
        "\n",
        "insert_rdd_to_db(rdd_titles, rdd_prices)\n"
      ],
      "metadata": {
        "id": "-W-UGAItfFJl"
      },
      "execution_count": 43,
      "outputs": []
    }
  ]
}